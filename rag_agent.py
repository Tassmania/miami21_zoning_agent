# ü§ñ rag_agent.py ‚Äî Retrieve Relevant Chunks + GPT Completion

# This script implements a simple RAG (Retrieval-Augmented Generation) agent
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.schema import HumanMessage, SystemMessage
# from langchain_openai import ChatOpenAI


# Load vector store
embedding = OpenAIEmbeddings(model="text-embedding-3-small")
vectordb = Chroma(persist_directory="miami21_vectorstore", embedding_function=embedding)

# LLM setup (OpenAI)
llm = ChatOpenAI(model="gpt-4", temperature=0)

SYSTEM_PROMPT = """
You are a zoning compliance advisor trained on the City of Miami‚Äôs Miami 21 zoning code.
Answer precisely based only on retrieved context. Do not speculate.
When responding, clarify whether the use is allowed by right, warrant, or exception,
and quote relevant rule, table, or supplemental regulation.
"""

def query_zoning(question: str, zone_code: str) -> str:
    try:
        zone_key = zone_code.strip().upper().replace(" ", "")  # Normalize

        # Search top 3 matching chunks
        results = vectordb.similarity_search(zone_key, k=3)
        context = "\n---\n".join([doc.page_content for doc in results])

        messages = [
            SystemMessage(content=SYSTEM_PROMPT),
            HumanMessage(content=f"Context:\n{context}\n\nUser Question: {question}")
        ]

        reply = llm(messages).content
        return reply

    except Exception as e:
        return f"‚ùå Error during zoning query: {e}"
